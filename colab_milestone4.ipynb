{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Temporal Transformer for Financial Fraud Detection\n",
        "**Colab Pro GPU-Optimized Notebook**\n",
        "\n",
        "This notebook runs the complete fraud detection pipeline on GPU with optimized settings for Colab Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies for Colab Pro\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q scikit-learn pandas matplotlib seaborn numpy\n",
        "print('âœ… Dependencies installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload and extract project files (since we can't clone from GitHub in this demo)\n",
        "# Alternative: Upload the project as a zip file to Colab\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# For demo purposes, we'll create the package structure directly\n",
        "!mkdir -p deep_temporal_transformer/{configs,data,models,training,evaluation,utils,examples}\n",
        "print('ðŸ“ Project structure created')\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.append('/content')\n",
        "print('ðŸ Python path updated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create minimal working version for Colab\n",
        "%%writefile deep_temporal_transformer/__init__.py\n",
        "\"\"\"Deep Temporal Transformer Package\"\"\"\n",
        "__version__ = \"1.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick demo with synthetic data (Colab-optimized)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸš€ Using device: {device}\")\n",
        "\n",
        "# Generate synthetic fraud data\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "n_samples = 10000  # Smaller for Colab\n",
        "n_features = 10\n",
        "fraud_rate = 0.02\n",
        "\n",
        "# Create synthetic transaction data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "y = np.random.binomial(1, fraud_rate, n_samples)\n",
        "\n",
        "# Add fraud patterns\n",
        "fraud_indices = np.where(y == 1)[0]\n",
        "X[fraud_indices, :3] += np.random.normal(2, 0.5, (len(fraud_indices), 3))\n",
        "\n",
        "print(f\"ðŸ“Š Generated {n_samples} samples with {y.sum()} fraud cases ({y.mean():.1%} fraud rate)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple transformer model for Colab\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=128, \n",
        "            batch_first=True, activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Add sequence dimension for transformer\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)  # (batch, 1, features)\n",
        "        \n",
        "        h = self.input_proj(x)\n",
        "        h = self.transformer(h)\n",
        "        h = h.mean(dim=1)  # Global average pooling\n",
        "        return self.classifier(h).squeeze(-1)\n",
        "\n",
        "print('ðŸ¤– Simple Transformer model defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"ðŸ“ˆ Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# Baseline: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "rf_pred = rf.predict(X_test_scaled)\n",
        "rf_prob = rf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "rf_f1 = f1_score(y_test, rf_pred)\n",
        "rf_auc = roc_auc_score(y_test, rf_prob)\n",
        "\n",
        "print(f\"ðŸŒ² Random Forest - F1: {rf_f1:.4f}, AUC: {rf_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Simple Transformer\n",
        "model = SimpleTransformer(input_dim=n_features).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(20.0).to(device))  # Handle imbalance\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(0, len(X_train_tensor), batch_size):\n",
        "        batch_X = X_train_tensor[i:i+batch_size]\n",
        "        batch_y = y_train_tensor[i:i+batch_size]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(range(0, len(X_train_tensor), batch_size)):.4f}\")\n",
        "\n",
        "print('ðŸŽ¯ Transformer training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Transformer\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_tensor)\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "\n",
        "transformer_f1 = f1_score(y_test, preds)\n",
        "transformer_auc = roc_auc_score(y_test, probs)\n",
        "\n",
        "print(f\"ðŸ¤– Simple Transformer - F1: {transformer_f1:.4f}, AUC: {transformer_auc:.4f}\")\n",
        "\n",
        "# Results comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“Š FRAUD DETECTION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Random Forest:     F1={rf_f1:.4f}, AUC={rf_auc:.4f}\")\n",
        "print(f\"Simple Transformer: F1={transformer_f1:.4f}, AUC={transformer_auc:.4f}\")\n",
        "print(\"\\nâœ… Colab Pro Demo Completed Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# ROC Curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_prob)\n",
        "fpr_tr, tpr_tr, _ = roc_curve(y_test, probs)\n",
        "\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={rf_auc:.3f})', linewidth=2)\n",
        "plt.plot(fpr_tr, tpr_tr, label=f'Transformer (AUC={transformer_auc:.3f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'Fraud'], \n",
        "            yticklabels=['Normal', 'Fraud'])\n",
        "plt.title('Transformer Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ“ˆ Visualization completed\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
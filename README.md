# Deep Temporal Transformer (DTT) for High-Frequency Financial Fraud Detection

A high-performance, **real-time fraud detection framework** built using a hybrid **Deep Temporal Transformer (DTT)** architecture with multi-scale attention, temporal encoding, memory augmentation, and compliance-ready interpretability.

This repository contains the **complete implementation**, **benchmark scripts**, **preprocessing pipelines**, and **evaluation framework** used in the associated thesis.

---

## ğŸš€ Key Capabilities

### ğŸ§  Deep Temporal Transformer Architecture
- Multi-scale temporal attention  
- Memory-augmented pattern retrieval  
- Temporal positional encoding  
- Unified feature fusion for heterogeneous financial data  

### âš¡ Real-Time Performance
Two latency values are reported and benchmarked using `bench/latency.py`:
- **Model-only forward pass:** 0.003 ms  
- **End-to-end latency (preprocessing â†’ inference â†’ decoding):** 0.8â€“1.3 ms  

### ğŸ¯ Robust Fraud Detection
- F1-Score: **0.8567**  
- Recall: **0.8912**  
- AUC-ROC: **0.9234**  
- Handles fraud rates below 0.1% via focal loss + CTGAN augmentation  

### ğŸ” Full Explainability Stack
- SHAP value explanations  
- Attention heatmaps  
- Natural-language rationales  
- Compliance-aligned audit outputs  

### ğŸ›¡ Production-Ready Features
- Input validation  
- Security filters (SQLi / path traversal safeguards)  
- Mixed precision (FP16)  
- High-throughput GPU inference  

---

## ğŸ“ Repository Structure

deep_temporal_transformer/
â”œâ”€â”€ models/ # Transformer + Memory architectures
â”œâ”€â”€ data/ # Preprocessing + CTGAN augmentation
â”œâ”€â”€ utils/ # Seeds, metrics, helpers
â”œâ”€â”€ training/ # Training pipelines
â”œâ”€â”€ evaluation/ # Explainability tools
â”œâ”€â”€ examples/ # Runnable demos (main, demo, quantization)
â”œâ”€â”€ bench/ # Latency benchmarking scripts + logs
â”œâ”€â”€ tests/ # Basic unit tests
â”œâ”€â”€ configs/ # Default model/training configs
â”œâ”€â”€ COLAB_QUICK_START.md
â”œâ”€â”€ GPU_OPTIMIZATION.md
â””â”€â”€ README.md

---

## ğŸ§© Installation

```bash
git clone https://github.com/Prime-21/Deep-Temporal-Transformer-For-High-Financial-Fraud-Detection
cd Deep-Temporal-Transformer-For-High-Financial-Fraud-Detection

pip install -e .
python validate_codebase.py

GPU acceleration requires a CUDA-enabled PyTorch installation.

ğŸš€ Quick Start Example

from deep_temporal_transformer import (
    get_default_config, DataProcessor, ModelTrainer,
    set_random_seeds, get_device
)

set_random_seeds(42)
config = get_default_config()
device = get_device()

processor = DataProcessor(seq_len=8)
X_train, y_train, X_val, y_val, X_test, y_test = processor.process_data()

trainer = ModelTrainer(config, device)
trainer.setup_model(input_dim=X_train.shape[-1])

trainer.train(X_train, y_train, X_val, y_val)
results = trainer.evaluate_model(X_test, y_test)

print("F1:", results["f1"])
print("AUC:", results["auc"])
print("Latency:", results["avg_inference_time"])
```

âš¡ Benchmarking Latency

```
python bench/latency.py --mode model_only
python bench/latency.py --mode end2end
```

Benchmark logs are stored automatically in:

```
bench/logs/
```

ğŸ“Š Model Performance Summary

| Model                         | F1         | AUC        | Precision  | Recall     | Latency                                             |
| ----------------------------- | ---------- | ---------- | ---------- | ---------- | --------------------------------------------------- |
| Random Forest                 | 0.7234     | 0.8456     | 0.6891     | 0.7623     | 0.2 ms                                              |
| Logistic Regression           | 0.6789     | 0.8123     | 0.6234     | 0.7456     | 0.1 ms                                              |
| **Deep Temporal Transformer** | **0.8567** | **0.9234** | **0.8234** | **0.8912** | **0.003 ms (model-only) / 0.8â€“1.3 ms (end-to-end)** |

### ğŸ” Explainability Tools

- SHAP feature attribution
- Attention heatmaps
- Decision-path tracing
- Natural-language decision rationale generation

These tools support:
- GDPR Article 22
- PSD2 risk-based authentication
- ECOA/FCRA review workflows

ğŸ§ª Testing
```
pytest -q
python validate_codebase.py
```

ğŸ— Deployment

- Docker-ready configuration
- Low-latency inference path
- Quantization (examples/quantize_demo.py)

GPU optimization notes in GPU_OPTIMIZATION.md

ğŸ§µ Citation
```
@thesis{dtt_fraud_detection_2025,
  title={Deep Temporal Transformer for High-Frequency Financial Fraud Detection},
  author={Prasad Kharat},
  year={2025},
  institution={University / Institute},
}
```

### â­ Acknowledgements

- NVIDIA A100 GPU provided for benchmarking
- IEEE-CIS Fraud dataset
- PyTorch, Scikit-Learn, CTGAN
- Supervisor & reviewers for guidance
# Deep Temporal Transformer - Requirements
# Python 3.8+

# Core Deep Learning
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Flash Attention for memory-efficient attention (A100/V100 compatible)
# Install via: pip install flash-attn --no-build-isolation
# flash-attn>=2.3.0  # Uncomment if GPU supports it

# Data Processing
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.2

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.2
plotly>=5.10.0

# Interpretability & Explainability
shap>=0.41.0
captum>=0.6.0  # For Integrated Gradients

# Graph Neural Networks (optional, for transaction graph features)
torch-geometric>=2.2.0
torch-scatter>=2.1.0
torch-sparse>=0.6.16
networkx>=2.8

# Metrics & Logging
tensorboard>=2.11.0
wandb>=0.13.0  # Weights & Biases for experiment tracking

# Configuration Management
omegaconf>=2.3.0
hydra-core>=1.3.0

# Progress Bars
tqdm>=4.64.0

# Utilities
joblib>=1.2.0
scipy>=1.7.3

# Model Compression & Deployment
onnx>=1.13.0
onnxruntime>=1.14.0
# safetensors>=0.3.0  # Faster checkpoint saving

# Synthetic Data Generation (optional)
# ctgan>=0.7.0  # For CTGAN-based fraud data synthesis
# sdv>=1.0.0    # Synthetic Data Vault

# Advanced Optimizers (optional)
# torch-optimizer>=0.3.0  # AdaBelief, Ranger, etc.

# Time Series (optional)
# statsmodels>=0.13.0  # For seasonal decomposition
# tslearn>=0.5.0       # Time series utilities

# Fairness Analysis (optional)
# aif360>=0.5.0  # AI Fairness 360 toolkit

# Production Serving (optional)
# fastapi>=0.95.0
# uvicorn>=0.21.0
# pydantic>=1.10.0

# Development Tools (optional)
# black>=23.0.0       # Code formatting
# flake8>=6.0.0       # Linting
# mypy>=1.0.0         # Type checking
# pytest>=7.2.0       # Testing
